{"name":"Mind-heart-reader","tagline":"","body":"#Project Goals and Research Questions - #\r\nWe intend to use the IAPS[1] and IADS[2] dataset to compare the patterns in brain waves in response to visual and aural stimuli. The key question is whether users’ emotional response to a certain stimulus differs based on the medium of that stimulus (in this case images v/s. sounds). \r\nWe will also build machine learning classifier which classifies the brainwaves and predicts emotional response for the user. Within each stimulus, we will prompt the user for a response (like, neutral, dislike).  After several iterations, we can train the classifier using bio-data inputs tagged with users responses. The classifier can then be used to predict a user’s response based on bio-data inputs.\r\n#Use Cases#\r\n\r\n#Interviews and Ideation Plan#\r\nWe will build an user interface which will display images and play sounds for the subject to rate them in one of the three categories - Like, Dislike and Neutral.\r\nWe will customize the interview methodology used in the IAPS and IADS studies which was a paper based version and asked users to rate the images and sound in a booklet. They closely follow the Self-assessment Manikin(SAM, Lang 1980) methodology. Parameters such as time the image/sound is provided to user are to be determined.\r\nFeasibility - \r\n\r\n##Dataset##\r\nThe two datasets are standardized and have been used in research extensively.  We aim to use subsets of each dataset that are equivalent to each other. The images and sounds used in each experiment will be selected carefully. We aim to use more extreme images along the emotional dimension the dataset has. Our plan is get image that are likely to illicit highly positive or highly negative emotion. Neutral images help control for both responses. This will , hopefully, reduce dimensionality of user responses and improve chances of predicting those responses.\r\n\r\n##Hardware:##\r\n**Mindwave:** We will be using the Mindwave device to capture brainwave data\r\nBasis Watch:  To capture heart rate data  \r\n##Software:##\r\n**Python/Web:** We are using a python web app (with flask) to conduct the experiments, analyze the data, and visualize the conclusions. This architecture allows us to combine both UI elements with backend data processing. Also, because timing is critical, we need to ensure we time user responds accurately to synchronize them with data inputs from the sensors. Using python also allows us to use the mindwave sensor in real time if possible. We have extended the indra-client module to allows us (and anyone else) to run it in parallel with our application. one downside to this is that we won’t be able to use NeuroSky metrics which need to be post-calculated.\r\n**Scikit-Learn: ** Python library for machine learning\r\nNeuroSky Apps: to capture and calculate mindwave data and metrics\r\n**D3:** for data visualization. (We used Tableau instead to focus on analysis instead of visualization)\r\nUser Testing and Prototype Iterations - Add any link to a prototype, add videos or pictures.\r\nBuild the technical backend to conduct the experiment and capture user responses.\r\nConduct several experiments informally within the I School friends and family network to capture bio data and responses\r\n# Prepare bio data for analysis:\r\n* calculate mindwave metrics and load them to the database\r\n* upload heart rate data to the database\r\n* Analyze the different data streams to:\r\n* compare stimulus responses for each user\r\n* build the prediction classifier\r\n* build information visualizations to simulate the experiment\r\n* Conduct additional interviews with the full system to verify the findings\r\nEvaluation - To be filled later with some representative data/statistics/graphs that prove you succeeded (or not!)\r\nFinal Portfolio Pieces / Pictures - To be added later.\r\n\r\n\r\n\f\r\n# Activities:\r\n## Image Selection:\r\n## Experiment App:\r\nWe built the app and backend database to display and capture user images\r\n## Process:\r\n* Create user-> create experiment with specific image/sound set -> connect sensors -> conduct -> upload sensor data.\r\n* Initially, we had a 20 second time window per file. We found that users were bored and anxious to proceed so we reduced it to 15 seconds but with 25 image. One hypothesis is that the data that matters is in the beginning of each window (i.e. the first impression).\r\n* We had issues mapping the times of user responses with mindwave data because of different timezone settings so we had to fix that programmatically.\r\n* We deployed the app on Amazon Web Services to allow all members access to it. However, the images/sounds were stored on a our secure iSchool server and accessed remotely from AWS.\r\n\r\n# Experiments:\r\n* Conducted several 10 complete experiments\r\n* 4 participants did both sound and image experiments\r\n* Conduced several failed experiments:\r\n* Sensors didn’t connect\r\n* NeuroSky platform didn’t start or crashed\r\n* Unable to obtain good quality signal before or during the experiment\r\nChanged experiment setup to be on two machines instead of one:\r\nmanaging the sensor platform and the experiment application was too distracting and confusing to the users. We prepared a dedicated laptop for the sensors and conducted experiment using isolated desktops in the co lab. This was also better ergonomically for the participants.\r\n# Visualization\r\nInitial plan was to build custom visualizations with D3 into the app. However, when we explored the data from early experiments visually using Tableau, we didn’t find any clear patterns. We decided to focus our time and effort doing visual analysis using Tableau instead of spending time on building custom visualization that might not show anything.\r\nWe had to customize the database data dump to make it more suitable for visual analysis with Tableau.\r\n\r\nClassification\r\n\r\nReferences\r\n[1] Lang, P.J., Bradley, M.M., & Cuthbert, B.N. (2008). International affective picture system (IAPS): Affective ratings of pictures and instruction manual. Technical Report A-8. University of Florida, Gainesville, FL.\r\n\r\n[2] Bradley, M. M., & Lang, P. J. (1999). International affective digitized sounds (IADS): Stimuli, instruction manual and affective ratings (Tech. Rep. No. B-2). Gainesville, FL: The Center for Research in Psychophysiology, University of Florida","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}